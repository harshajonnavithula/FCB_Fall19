{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from twython import Twython\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from IPython.display import Image as im\n",
    "\n",
    "import tweepy\n",
    "\n",
    "consumer_key = \"98HildnXniXArE0g4y0rxHvy6\"\n",
    "consumer_secret = \"KuzGtjnGmilL8MoHzUjBuAWykVQlqgJmm6kBxFGxOlw4Y84cWq\"\n",
    "access_token = \"415927378-SXuiQmVewRzdztfXPPgeboeqfPk8QvMB4AmiTwll\"\n",
    "access_token_secret = \"b3VX2jBGFlZKQFpMFCQcme39JjiDVtpm3uFIXXSeHkLul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt authentication \n",
    "try: \n",
    "    # Creating the authentication object\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    # Setting your access token and secret\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # Creating the API object while passing in auth information\n",
    "    api = tweepy.API(auth)\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the API object to get tweets from your timeline, and storing it in a variable called public_tweets\n",
    "public_tweets = api.home_timeline()\n",
    "# foreach through all tweets pulled\n",
    "# for tweet in public_tweets:\n",
    "   # printing the text stored inside the tweet object\n",
    "   # print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The search term you want to find\n",
    "query = \"Food and Travel\"\n",
    "# Language code (follows ISO 639-1 standards)\n",
    "language = \"en\"\n",
    "\n",
    "# Calling the user_timeline function with our parameters\n",
    "query_results = api.search(q=query, lang=language, count = 5)\n",
    "\n",
    "# foreach through all tweets pulled\n",
    "# for tweet in query_results:\n",
    "   # printing the text stored inside the tweet object\n",
    "   # print(tweet.user.screen_name,\"Tweeted:\",tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for trends using the WOEID of the location\n",
    "trend_results = api.trends_place(id = 2379574)\n",
    "\n",
    "# for location in trend_results:\n",
    "#     for trend in location[\"trends\"]:\n",
    "        # print(trend[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The search term you want to find\n",
    "query = \"#food\"\n",
    "# Language code (follows ISO 639-1 standards)\n",
    "language = \"en\"\n",
    "\n",
    "# Calling the user_timeline function with our parameters\n",
    "#query_results = api.search(q=query, lang=language, count=10000, since_id=\"2019-01-01\")\n",
    "#query_results = tweepy.Cursor(api.search,q=query, lang=language, since_id=\"2019-10-14\", wait_on_rate_limit=True).items()\n",
    "\n",
    "food_tweets = []\n",
    "\n",
    "# foreach through all tweets pulled\n",
    "#for tweet in query_results:\n",
    "    # printing the text stored inside the tweet object\n",
    "    # print(tweet.user.screen_name,\"Tweeted:\",tweet.text)\n",
    "    #food_tweets.append(tweet.text)\n",
    "    #print (tweet.created_at, tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "# foodDict = dict()\n",
    "\n",
    "# for tweet in food_tweets:\n",
    "#     for word in words:\n",
    "#         if word in tweet.lower():\n",
    "#             if word in foodDict.keys():\n",
    "#                 foodDict[word] += 1\n",
    "#             else:\n",
    "#                 foodDict[word] = 1\n",
    "\n",
    "# food_df = pd.DataFrame(list(foodDict.items()), columns=['Word', 'Count'])\n",
    "# print(len(food_tweets))\n",
    "# food_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# Chefs Accounts\n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('Chefs.txt', 'r') as f:\n",
    "    chefs = f.readlines()\n",
    "    \n",
    "# Remove '\\n' from the text\n",
    "chefs_usernames = [x.strip() for x in chefs]\n",
    "# Get the usernames of the chefs\n",
    "chefs_usernames = [x.split(\"- \",1)[1] for x in chefs_usernames]\n",
    "\n",
    "# Initialize an empty list\n",
    "chef_tweets = []\n",
    "\n",
    "# Get the tweets of all chefs extracted from the text file\n",
    "for aChef in chefs_usernames:\n",
    "    try:\n",
    "        # Get chef tweets\n",
    "        data = tweepy.Cursor(api.user_timeline, id=aChef, wait_on_rate_limit=True).items()\n",
    "        # data = api.user_timeline(aChef, count = 100)\n",
    "        for status in data:\n",
    "            # if (\"2019\" in status._json[\"created_at\"]):\n",
    "            # tweets.append(status._json[\"text\"].lower())\n",
    "            if (status.created_at.year >= 2016):\n",
    "                chef_tweets.append(status.text.lower())\n",
    "                print(status.user.screen_name, status.created_at, status.text)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "f.close()\n",
    "print(len(chefs))\n",
    "print(len(chef_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write All Chefs Tweets To a File\n",
    "with open(\"ChefsTweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    for tweet in chef_tweets:\n",
    "        f.write(tweet)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('Food_Keywords.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDict = dict()\n",
    "\n",
    "for tweet in chef_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDict.keys():\n",
    "                wordDict[word] += 1\n",
    "            else:\n",
    "                wordDict[word] = 1\n",
    " \n",
    "df = pd.DataFrame(list(wordDict.items()), columns=['Word', 'Count'])\n",
    "sorted_df = df.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "#sorted_df\n",
    "#subset_df = sorted_df[0:10]\n",
    "#subset_df.plot.bar(x='Word', y='Count', rot=0)\n",
    "\n",
    "sorted_df.to_csv(\"Chef_Keywords_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "# Food Accounts\n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('FoodAccounts.txt', 'r') as f:\n",
    "    food_accounts = f.readlines()\n",
    "    \n",
    "# Remove '\\n' from the text\n",
    "food_accounts_usernames = [x.strip() for x in food_accounts]\n",
    "# Get the usernames of the chefs\n",
    "food_accounts_usernames = [x.split(\"- \",1)[1] for x in food_accounts_usernames]\n",
    "\n",
    "# Initialize an empty list\n",
    "food_accounts_tweets = []\n",
    "\n",
    "# Get the tweets of all chefs extracted from the text file\n",
    "for foodAccount in food_accounts_usernames:\n",
    "    try:\n",
    "        # Get food account tweets\n",
    "        data = tweepy.Cursor(api.user_timeline, id=foodAccount, wait_on_rate_limit=True).items()\n",
    "        for status in data:\n",
    "            if (status.created_at.year >= 2016):\n",
    "                food_accounts_tweets.append(status.text.lower())\n",
    "                print(status.user.screen_name, status.created_at, status.text)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "f.close()\n",
    "print(len(food_accounts_usernames))\n",
    "print(len(food_accounts_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write All Food Accounts Tweets To a File\n",
    "with open(\"FoodAccountsTweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    for tweet in food_accounts_tweets:\n",
    "        f.write(tweet)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('Food_Keywords.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDictForFoodAccounts = dict()\n",
    "\n",
    "for tweet in food_accounts_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDictForFoodAccounts.keys():\n",
    "                wordDictForFoodAccounts[word] += 1\n",
    "            else:\n",
    "                wordDictForFoodAccounts[word] = 1\n",
    " \n",
    "foodAcconts_df = pd.DataFrame(list(wordDictForFoodAccounts.items()), columns=['Word', 'Count'])\n",
    "sorted_foodAcconts_df = foodAcconts_df.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "#sorted_df\n",
    "#subset_df = sorted_df[0:10]\n",
    "#subset_df.plot.bar(x='Word', y='Count', rot=0)\n",
    "\n",
    "sorted_foodAcconts_df.to_csv(\"Food_Accounts_Keyword_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_foodAcconts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a string form of our list of text\n",
    "all_tweets = chef_tweets + food_accounts_tweets\n",
    "raw_string = ''.join(all_tweets)\n",
    "no_links = re.sub(r'http\\S+', '', raw_string)\n",
    "no_unicode = re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', no_links)\n",
    "no_special_characters = re.sub('[^A-Za-z ]+', '', no_unicode)\n",
    "\n",
    "words = no_special_characters.split(\" \")\n",
    "words = [w for w in words if len(w) > 2]  # ignore a, an, be, ...\n",
    "words = [w.lower() for w in words]\n",
    "words = [w for w in words if w not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"C:/Users/Ahmed/Desktop/CGVT/FCB/Twitter Analysis/FCB.png\"))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, width=800, height=400, mask=mask)\n",
    "clean_string = ','.join(words)\n",
    "wc.generate(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import re\n",
    " \n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    " \n",
    "from operator import itemgetter\n",
    "\n",
    "WNL = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareStopWords():\n",
    "    stopwordsList = []\n",
    "    # Load default stop words and add a few more specific to my text.\n",
    "    stopwordsList = stopwords.words('english')\n",
    "    stopwordsList.append('dont')\n",
    "    stopwordsList.append('didnt')\n",
    "    stopwordsList.append('doesnt')\n",
    "    stopwordsList.append('cant')\n",
    "    stopwordsList.append('couldnt')\n",
    "    stopwordsList.append('couldve')\n",
    "    stopwordsList.append('im')\n",
    "    stopwordsList.append('ive')\n",
    "    stopwordsList.append('isnt')\n",
    "    stopwordsList.append('theres')\n",
    "    stopwordsList.append('wasnt')\n",
    "    stopwordsList.append('wouldnt')\n",
    "    stopwordsList.append('a')\n",
    "    stopwordsList.append('also')\n",
    "    stopwordsList.append('rt')\n",
    "    stopwordsList.append('de')\n",
    "    stopwordsList.append('la')\n",
    "    stopwordsList.append('join')\n",
    "    stopwordsList.append('like')\n",
    "    stopwordsList.append('day')\n",
    "    return stopwordsList\n",
    "\n",
    "# Open the file and read lines\n",
    "# NOTE: You need to give finder.score_ngrams a sizable corpus to work with.\n",
    " \n",
    "input_file = 'AllTweets.txt'\n",
    "FILEHEADER = 0\n",
    " \n",
    "with open(input_file, 'r', encoding=\"utf-8\") as f:\n",
    "    if FILEHEADER:\n",
    "        next(f)\n",
    "    rawText = f.read()\n",
    "\n",
    "# Lowercase and tokenize\n",
    "rawText = rawText.lower()\n",
    "\n",
    "# Remove single quote early since it causes problems with the tokenizer.\n",
    "# wasn't turns into 2 entries; was, n't.\n",
    "rawText = rawText.replace(\"'\", \"\")\n",
    "\n",
    "# Keep only english words\n",
    "words = set(nltk.corpus.words.words())\n",
    "rawText = \" \".join(w for w in nltk.wordpunct_tokenize(rawText) \\\n",
    "         if w.lower() in words or not w.isalpha())\n",
    "\n",
    "tokens = nltk.word_tokenize(rawText)\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Load default stop words and add a few more.\n",
    "stopWords = prepareStopWords()\n",
    "\n",
    "# Remove extra chars and remove stop words.\n",
    "text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text]\n",
    "text_content = [word for word in text_content if word not in stopWords]\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', word) for word in text_content]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', word) for word in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', word) for word in no_unicode]\n",
    "\n",
    "text_content = no_special_characters\n",
    "\n",
    "# After the punctuation above is removed it still leaves empty entries in the list.\n",
    "# Remove any entries where the len is zero.\n",
    "text_content = [s for s in text_content if len(s) != 0]\n",
    " \n",
    "# Best to get the lemmas of each word to reduce the number of similar words\n",
    "# on the word cloud. The default lemmatize method is noun, but this could be\n",
    "# expanded.\n",
    "# ex: The lemma of 'characters' is 'character'.\n",
    "text_content = [WNL.lemmatize(t) for t in text_content]\n",
    "\n",
    "# setup and score the bigrams using the raw frequency.\n",
    "finder = BigramCollocationFinder.from_words(text_content)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    " \n",
    "# By default finder.score_ngrams is sorted, however don't rely on this default behavior.\n",
    "# Sort highest to lowest based on the score.\n",
    "scoredList = sorted(scored, key=itemgetter(1), reverse=True)\n",
    "\n",
    "# word_dict is the dictionary we'll use for the word cloud.\n",
    "# Load dictionary with the FOR loop below.\n",
    "# The dictionary will look like this with the bigram and the score from above.\n",
    "# word_dict = {'bigram A': 0.000697411,\n",
    "#             'bigram B': 0.000524882}\n",
    " \n",
    "word_dict = {}\n",
    "\n",
    "listLen = len(scoredList)\n",
    "\n",
    "# Get the bigram and make a contiguous string for the dictionary key. \n",
    "# Set the key to the scored value. \n",
    "for i in range(listLen):\n",
    "    word_dict['_'.join(scoredList[i][0])] = scoredList[i][1]\n",
    "\n",
    "# Set word cloud params and instantiate the word cloud.\n",
    "# The height and width only affect the output image file.\n",
    "WC_background = 'white'\n",
    "WC_height = 400\n",
    "WC_width = 800\n",
    "WC_max_words = 100\n",
    " \n",
    "wordCloud = WordCloud(background_color=WC_background, max_words=WC_max_words, height=WC_height, width=WC_width)\n",
    "wordCloud.generate_from_frequencies(word_dict)\n",
    "\n",
    "plt.title('Bigram')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wordCloud.to_file(\"WordCloud_Bigrams_Frequent_Words.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict\n",
    "df = pd.DataFrame.from_dict(word_dict, orient=\"index\")\n",
    "df.to_csv(\"Bigram_All_Tweets_frequencyScore_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('ChefsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    chef_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', x) for x in chef_tweets]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', x) for x in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', x) for x in no_unicode]\n",
    "chef_tweets = no_special_characters\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('Top_Ingredients.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDict = dict()\n",
    "\n",
    "for tweet in chef_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDict.keys():\n",
    "                wordDict[word] += 1\n",
    "            else:\n",
    "                wordDict[word] = 1\n",
    " \n",
    "df = pd.DataFrame(list(wordDict.items()), columns=['Word', 'Count'])\n",
    "sorted_df = df.sort_values(by=['Count'], ascending=False)\n",
    "sorted_df.to_csv(\"Top_Ingredients_Chef_Keywords_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('ChefsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    chef_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', x) for x in chef_tweets]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', x) for x in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', x) for x in no_unicode]\n",
    "chef_tweets = no_special_characters\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('Top_Recipe_Titles.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDict = dict()\n",
    "\n",
    "for tweet in chef_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDict.keys():\n",
    "                wordDict[word] += 1\n",
    "            else:\n",
    "                wordDict[word] = 1\n",
    " \n",
    "df = pd.DataFrame(list(wordDict.items()), columns=['Word', 'Count'])\n",
    "sorted_df = df.sort_values(by=['Count'], ascending=False)\n",
    "sorted_df.to_csv(\"Top_Recipe_Titles_Chef_Keywords_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('FoodAccountsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    food_accounts_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', x) for x in food_accounts_tweets]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', x) for x in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', x) for x in no_unicode]\n",
    "food_accounts_tweets = no_special_characters\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('Top_Ingredients.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDictForFoodAccounts = dict()\n",
    "\n",
    "for tweet in food_accounts_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDictForFoodAccounts.keys():\n",
    "                wordDictForFoodAccounts[word] += 1\n",
    "            else:\n",
    "                wordDictForFoodAccounts[word] = 1\n",
    " \n",
    "df = pd.DataFrame(list(wordDictForFoodAccounts.items()), columns=['Word', 'Count'])\n",
    "sorted_df = df.sort_values(by=['Count'], ascending=False)\n",
    "sorted_df.to_csv(\"Top_Ingredients_Food_Accounts_Keywords_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the names and twitter usernames from a text file\n",
    "with open('FoodAccountsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    food_accounts_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', x) for x in food_accounts_tweets]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', x) for x in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', x) for x in no_unicode]\n",
    "food_accounts_tweets = no_special_characters\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('Top_Recipe_Titles.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Remove '\\n' from the text\n",
    "words = [x.strip() for x in words]\n",
    "# Make all words lowercase\n",
    "words = [x.lower() for x in words]\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "wordDictForFoodAccounts = dict()\n",
    "\n",
    "for tweet in food_accounts_tweets:\n",
    "    for word in words:\n",
    "        if word in tweet.lower():\n",
    "            if word in wordDictForFoodAccounts.keys():\n",
    "                wordDictForFoodAccounts[word] += 1\n",
    "            else:\n",
    "                wordDictForFoodAccounts[word] = 1\n",
    " \n",
    "df = pd.DataFrame(list(wordDictForFoodAccounts.items()), columns=['Word', 'Count'])\n",
    "sorted_df = df.sort_values(by=['Count'], ascending=False)\n",
    "sorted_df.to_csv(\"Top_Recipe_Titles_Food_Accounts_Keywords_Results.csv\", sep='\\t', encoding='utf-8')\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('ChefsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    chef_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Read the names and twitter usernames from a text file\n",
    "with open('FoodAccountsTweets.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    food_accounts_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "all_tweets = chef_tweets + food_accounts_tweets\n",
    "all_tweets = ' '.join(all_tweets)\n",
    "all_tweets = all_tweets.lower()\n",
    "all_tweets = all_tweets.split()\n",
    "\n",
    "def prepareStopWords():\n",
    "    stopwordsList = []\n",
    "    # Load default stop words and add a few more specific to my text.\n",
    "    stopwordsList = stopwords.words('english')\n",
    "    stopwordsList.append('dont')\n",
    "    stopwordsList.append('didnt')\n",
    "    stopwordsList.append('doesnt')\n",
    "    stopwordsList.append('cant')\n",
    "    stopwordsList.append('couldnt')\n",
    "    stopwordsList.append('couldve')\n",
    "    stopwordsList.append('im')\n",
    "    stopwordsList.append('ive')\n",
    "    stopwordsList.append('isnt')\n",
    "    stopwordsList.append('theres')\n",
    "    stopwordsList.append('wasnt')\n",
    "    stopwordsList.append('wouldnt')\n",
    "    stopwordsList.append('a')\n",
    "    stopwordsList.append('also')\n",
    "    stopwordsList.append('rt')\n",
    "    stopwordsList.append('de')\n",
    "    stopwordsList.append('la')\n",
    "    stopwordsList.append('join')\n",
    "    stopwordsList.append('like')\n",
    "    stopwordsList.append('day')\n",
    "    stopwordsList.append('amp')\n",
    "    return stopwordsList\n",
    "\n",
    "stops = prepareStopWords()\n",
    "\n",
    "no_links = [re.sub(r'http\\S+', '', x) for x in all_tweets]\n",
    "no_unicode = [re.sub(r\"\\\\[a-z][a-z]?[0-9]+\", '', x) for x in no_links]\n",
    "no_special_characters = [re.sub('[^A-Za-z ]+', '', x) for x in no_unicode]\n",
    "filtered_words = [word.lower() for word in no_special_characters if word not in stops]\n",
    "\n",
    "raw_string = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "response = tfidf.fit_transform([raw_string])\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "scoresDict = dict()\n",
    "\n",
    "for col in response.nonzero()[1]:\n",
    "    #print (feature_names[col], ' - ', response[0, col])\n",
    "    scoresDict[feature_names[col]] = response[0, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(scoresDict.items()), columns=['Word', 'Score'])\n",
    "sorted_df = df.sort_values(by=['Score'], ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.to_csv(\"TF_IDF_Results.csv\", sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
